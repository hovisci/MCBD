---
title: "MCBD Raw Survey Prep"
author: "Veronica F. Frans, CSIS Lab, Michigan State University"
date: "May 7, 2020 (Updated: September 4, 2020)"
output:
  html_document:
    keep_tex: yes
    toc: yes
    toc_depth: 4
    toc_float: true
    df_print: paged
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE, cache.comments = FALSE,
                      warning = FALSE, message = FALSE, results='hold')
```

# 1. Methods summary

Compilation, inspection, and rough formatting of the raw Google form responses from the Biodiversity and Metacoupling literature review. The reviewed articles are those that were accepted in the abstract screening process.

The outputs from this script are:

- Tables of extracted data fields containing errors for reviewers to manually edit (and later incorporated in updated versions of this R script).
- Surveys 1 - 3 for the 5 shared articles that were examined by all reviewers ("common papers").
- Surveys 1 - 3 for all remaining articles, to be cleaned in other R scripts.

# 2. R Setup

The script presented here was done using R (version 4.0.2; R Core Team 2020) and its packages.

Load libraries, directories, and custom functions from source file.

```{r}
# Source file
  source('./scripts/Reference.R')
```

Data is stored here:

```{r, echo=FALSE}
# Data directory
  dat.dir
```

Final tables are stored here:

```{r, echo=FALSE}
# Final tables
  tab.dir
```

Final figures are stored here:

```{r, echo=FALSE}
# Final figures
  fig.dir
```

For **this run** of the script, tables for manual checks will be stored in the following folder:

```{r, echo=FALSE}
tab.check.dir
```


# 3. Load data

```{r}
# load previous workspace (if needed)
  #load("RawSurveyPrep.RData")

# surveys
  survey1 <- read.csv(paste0(dat.dir,'rawData\\',
                '1. Criteria Coding (Responses) - Form Responses 1.csv'))
  survey2 <- read.csv(paste0(dat.dir,'rawData\\',
                '2. Metacoupling Biodiversity Coupling Coding (Responses) - Form Responses 1.csv'))
  survey3 <- read.csv(paste0(dat.dir,'rawData\\',
                '3. Metacoupling impact on biodiversity metric(s) (Responses) - Form Responses 1.csv'))
  
# surveys in Chinese (E_Xing)
  cnS1 <- read.csv(paste0(dat.dir,'rawData\\survey1_cn_processed.csv'))
  cnS2 <- read.csv(paste0(dat.dir,'rawData\\survey2_cn_processed.csv'))
  cnS3 <- read.csv(paste0(dat.dir,'rawData\\survey3_cn_processed.csv'))
  cn_edit <- read.csv(paste0(dat.dir,'rawData\\cn_survey_amendments.csv'))
  
# manual edits
  s1_manual <- read.csv(paste0(dat.dir,'rawData\\survey1_manual_edits.csv'))
```

# 4. Data cleanup

## 4.1 Survey 1

Goals:
sort by paper ID and timestamp
Take the latest survey for that paper ID
Select the unique paper ID per person that was done the most recently
i.e. no duplicate paper IDs, but take the most recent edits they did

Error check:
  - Need to double-check paper IDs and match with author names and years
  - need to import the final paper list with assignments for checking this

Quality check: separate the 5 papers that everyone did
  - use the final answers from these to also control the other survey column answers
  
Error check:
  - ensure that the 'no's' don't have any answers for surveys 2 or 3
  - ensure that all yes' have answers for 2 or 3
  - ensure that any updated no's (and for sure a NO) have surveys 2 or 3 REMOVED from the dataset

### 4.1.1 Data preview

The summaries here are **preliminary**, as the final numbers depend on the edits.

```{r}
# structure
  #str(survey1)

# number entries
  paste('number of entries:',nrow(survey1))
  
# number columns
  paste('number of columns:',ncol(survey1))
  
# column names
  #colnames(survey1)

# number of unique papers surveyed
  paste('number of papers:',length(unique(survey1$Paper.ID.Code)))
  
# number of observers
  paste('number of observers:',length(unique(survey1$Coder.ID)))
  
# list of observers
  paste('observer names:')
  paste(unique(survey1$Coder.ID),collapse="; ")
```

The expected number of papers surveyed should be **593** (plus or minus 2 with duplicated study IDs).

### 4.1.2 Edit study IDs from manual edit file

Some study IDs in survey 1 were written incorrectly. These were manually edited by MG_Chung in a new table. We add these edits here to change the survey numbers.

First, select only the timestamp, coder ID, only paper ID and correction, and display the errors. 

```{r}
# select columns
  s1_manual <- subset(s1_manual,
                      select = c('Timestamp','Coder.ID','Paper_ID_correction', 
                                 'Paper.ID.Code', 'First.Author.s.Last.Name'))
# show edits
  s1_manual
```

How many are there, and are all values unique? Duplicates would be a little more complicated...

```{r}
# count
  nrow(s1_manual)

# count unique
  length(unique(s1_manual$Paper.ID.Code))
```

Next, extract these entries from survey 1 as a new subset.

```{r}
# subset
  s1_sub <- survey1 %>%
              filter(Paper.ID.Code %in% s1_manual$Paper.ID.Code) %>% 
              filter(Coder.ID %in% s1_manual$Coder.ID) %>% 
              # manually remove some extras
              filter(First.Author.s.Last.Name!='Pour') %>% 
              filter(First.Author.s.Last.Name!='Gallant') %>% 
              filter(Timestamp!='8/31/2020 14:35:40')


# ensure list of s1_sub matches list of s1_manual
  a <- s1_sub %>% arrange(Paper.ID.Code) %>% subset(select=c('Paper.ID.Code'))
  b <- s1_manual %>% arrange(Paper.ID.Code) %>% subset(select=c('Paper.ID.Code'))
  cat('test:\n')
  paste(a==b)
```

Remove from the previous dataset (will append later)

```{r}
# remove from survey 1
  survey1 <- anti_join(survey1,s1_sub)
```

Edit paper IDs of the subset
```{r}
# edit paper IDs (NOTE: this edit requires that positions of wrong and right code to be the same!!)
  wrong_code <- s1_manual %>% arrange(Paper.ID.Code) %>% subset(select=c('Paper.ID.Code'))
  right_code <- s1_manual %>% arrange(Paper.ID.Code) %>% subset(select=c('Paper_ID_correction'))
  # convert to vector
    wrong_code <- wrong_code$Paper.ID.Code          
    right_code <- right_code$Paper_ID_correction
  # replace values
    s1_sub$Paper.ID.Code <- mapvalues(s1_sub$Paper.ID.Code,
                                      from=wrong_code,to=right_code)
```

Rejoin to survey 1

```{r}
# join edited surveys to survey 1
  survey1 <- rbind(survey1,s1_sub)
```

Quick edit of Paper ID mismatch from A_Herzberger and R_Chen.

```{r}
# paper ID manual edits
  survey1$Paper.ID.Code[survey1$Paper.ID.Code==2001] <- 2201
  # survey1$Paper.ID.Code[survey1$Paper.ID.Code==264 & 
  #                         survey1$Coder.ID=='R_Chen'] <- 2800
```

We also found an article mistakenly surveyed. This will be removed from all analyses, as of a decision on 5-15-2020.

```{r}
# drop entries completely (invalid papers)
  survey1 <- survey1[!survey1$Paper.ID.Code==6005,]
```

We will also remove an observer from survey 1, who only did part of one of the common surveys, and another observer whose papers were full of errors and had to be completely redone. This prevents skewing agreement across the common surveys, and from using other invalidated surveys that have been redone.

```{r}
# drop entries completely (invalid papers)
  survey1 <- survey1[!survey1$Coder.ID=='V_Frans',]
  survey1 <- survey1[!survey1$Coder.ID=='R_Chen',]
```


### 4.1.3 Edit columns

Drop empty columns in the dataset

```{r}
# drop blank columns
  survey1 <-  subset(survey1, select = -c(X, X.1) )
```

Edit column names (too long!)

```{r}
# get colnames
  #colnames(survey1)

# change colnames (order matters!!!!!!!!)
  colnames(survey1) <- c("timestamp",
                         "coder_id",
                         "paper_id",
                         "year_pub",
                         "author",
                         "C1_biodiv",
                         "C2_meta",
                         "C3_quant",
                         "C4_study",
                         "meet_all_4",
                         "sup_check",
                         "comments",
                         "further_discuss",
                         "explicit_distant_impacts",
                         "explain_distant_impacts") 
```

Change structure (if needed)

```{r}
# check structure
  #str(survey1)
```

### 4.1.4 Combine with Chinese version of the survey entries

First, convert timestamps, since different formats.

```{r}
# convert timestamp to POSIX format
  survey1$timestamp <- lubridate::mdy_hms(as.character(survey1$timestamp),
                                          truncated=3)

  cnS1$timestamp <- lubridate::ymd_hms(as.character(cnS1$timestamp),
                                          truncated=3)
  #str(survey1$timestamp)
```

Then, check column names.

```{r}
# check column name matching. If mismatch is listed, correct before binding
  cat('column mismatch test:\n')
  setdiff(colnames(survey1),colnames(cnS1))
```

Quick edit of Paper ID mismatches from E_Xing.

```{r}
# paper ID manual edits
  cnS1$paper_id[cnS1$paper_id==3879] <- 3897
  cnS1$paper_id[cnS1$paper_id==3881] <- 3888
```

Quick edit of missing entries from surveys 1 and 3.

```{r}
# select columns
  cns1_manual <- subset(cn_edit,
                        select = c('paper_id',
                                   'explicit_distant_impacts',
                                   'explain_distant_impacts'))
# show edits
  #cns1_manual

# subset from cn1 survey
  cns1_sub <- cnS1 %>%
                arrange(paper_id) %>%
                  filter(paper_id %in% cns1_manual$paper_id)

# ensure list of s1_sub matches list of s1_manual
  a <- cns1_sub %>% arrange(paper_id) %>% subset(select=c('paper_id'))
  b <- cns1_manual %>% arrange(paper_id) %>% subset(select=c('paper_id'))
  cat('test:\n')
  paste(a==b)
 
# remove from cn survey 1
  cnS1 <- anti_join(cnS1,cns1_sub)
 
# edit entries by simply appending
  cns1_sub$explicit_distant_impacts <- cns1_manual$explicit_distant_impacts
  cns1_sub$explain_distant_impacts <- cns1_manual$explain_distant_impacts
    
# join edited surveys to survey 1
  cnS1 <- rbind(cnS1,cns1_sub)
```

Bind rows.

**NOTE:** structure for those matching column names have to be the same; also, any missing columns will be filled with NA.

```{r}
# combine
  survey1 <- bind_rows(survey1,cnS1)
```

### 4.1.5 Edit name errors

Name formatting matters for sorting data by timestamp. We will repeat this for the other surveys, too. 

```{r}
# change dashes to underscores
  survey1$coder_id <- gsub("-", "_", survey1$coder_id)

# change first to cap and last names to cap and lower case
  survey1$coder_id <- str_to_title(gsub("_"," ",survey1$coder_id))
  survey1$coder_id <- gsub(" ","_",survey1$coder_id)
  survey1$coder_id <- gsub("Mg","MG",survey1$coder_id)

# show updated list
  paste('observer names:')
  paste(unique(survey1$coder_id),collapse="; ")

```

### 4.1.6 Format dates and extract unique papers

We will repeat this for the other surveys, too. 

Sort and extract entry by latest timestamp. The resulting number of entries should match the number of unique study IDs across coders.

```{r}
# get original length
  a <- length(unique(c(survey1$paper_id,survey1$coder_id)))

# select latest data entries according to paper ID, observer and timestamp
  survey1 <-  survey1 %>% 
                # Within each grouping of col 1 and col 2...
                group_by(paper_id, coder_id) %>% 
                # Sort rows by descending order of timestamp...
                # (extra step helpful in checking what happens here)
                arrange(paper_id, coder_id, desc(timestamp)) %>% 
                # Pick the latest time
                slice(which.max(timestamp)) %>% 
                # ungroup
                ungroup()

# check if length matches
  b <- length(unique(c(survey1$paper_id,survey1$coder_id)))
  paste('test:', a == b)
  
# how many?
  paste('count:', b)
```

### 4.1.7 Make lists of accepted and rejected papers for Survey 1

We are only using the following question from survey 1 to confirm that a paper is accepted (core team decision):

*Were you able to explicitly indicate that the paper meets all four of our criteria for the previous questions?*

Any entries mistakenly put as 'yes' when really not eligible will be changed via R later, after reviewing surveys 2 and 3.

Here, we are also including manual rejections from team meeting discussions of papers on 5-1-2020, 5-13-2020 6-4-2020, and those via email.

```{r}
# change paper id field to character
  survey1$paper_id <- as.character(survey1$paper_id)

# list of papers to manually reject
  manual <- c(# based on survey 1 discussion
                1377, 3738,332,4841,    # A_Herzberger papers
                411, 922,               # A_Torres papers
                216, 259, 1253, 5576,   # E_Dean papers
                5032,
                5497, 4528, 4733, 2723, # K_Kapsar papers
                6276, 5589,             # M_Lei papers
                6535, 3502,             # MG_Chung papers
                #4352, 2036, 7010,2800, # R_Chen papers
                92,                     # Y_Dou papers
                1,94,203,770,           # Y_Li papers
                64,225, 619, 718,
                1879, 7011, 2427,       # Y_Zhang papers
              # based on survey 2 discussion
                1717, 1676,              # E_Dean papers
              
                4132,                                   # Manual reject based on Animal Mig fixes (CLH 9/23/2020)
                4207,6493,2704,1752,3335,109            # Manual reject based on tele_cat Aurora input (CLH 9/29/20) 
              )

# make list of ineligible paper ID numbers and remove from survey
  s1_keep <- survey1 %>%
             # meeting all 4 criteria
               subset(meet_all_4!='No') %>%
             # manual rejections (based on meeting)
               subset(!grepl(paste(manual,collapse="$|^"), 
                           paper_id)) %>% 
             # manual rejections (based on meeting)
               subset((paper_id!=1488 & coder_id!='Y_Li')|
                        (coder_id!='Y_Li'& paper_id!=3924))

# subset separately for some other errors
  s1_keep2 <- survey1 %>% 
              subset((coder_id=='Y_Li'& paper_id==813)|
                      (coder_id=='Y_Li'& paper_id==6655)|
                      (coder_id=='Y_Li'& paper_id==2222))
  
# combine
  s1_keep <- rbind(s1_keep,s1_keep2) 
  
# anti-join to make list of rejected papers
  s1_reject <- anti_join(survey1,s1_keep)
  
# show length of accepted and rejected
  paste('number accepted papers:', length(unique(s1_keep$paper_id)));
  paste('number rejected papers:', length(unique(s1_reject$paper_id)))
  
# check if length of objects match survey form
  paste('test:', nrow(s1_keep)+nrow(s1_reject) == nrow(survey1))
```

Make a new survey1 with accept/reject column (called status) for later query if needed. Also change the answer for 'meet_all_4' to NO for s1_reject.

```{r}
# add column
  s1_keep$status <- 'accept'
  s1_reject$status <- 'reject'
  
# change 'meet_all_4' question
  s1_reject$meet_all_4 <- 'No'

# rbind
  survey1 <- rbind(s1_keep,s1_reject)
```

Extract list of unique papers and observer IDs to CSV for core team use (will cbind later).

```{r}
# list of completed surveys
  s1_complete <- survey1 %>% 
                 arrange(coder_id, paper_id, desc(timestamp)) %>% 
                 subset(select=c(timestamp,coder_id,paper_id))

# summaries per person
  t1 <- ddply(survey1,
            .(coder_id),
            summarize,
            # total count of papers per person     
            s1_papers=length(unique(paper_id)),
            # number rejected papers
            s1_reject=sum(status=='reject'),
            # number accepted papers
            s1_accept=sum(status=='accept'),
            # calc acceptance rate
            acc_rate=round(sum(status=='accept')/length(coder_id), digits=4),
            # last date someone entered data
            s1_last_date=max(timestamp)
            )
  
# save as CSVs
  write.csv(s1_complete,
            paste0(tab.check.dir,'survey1_papers_observers.csv'),
            row.names = FALSE)
  write.csv(t1,
            paste0(tab.check.dir,'survey1_progress.csv'),
            row.names = FALSE)
  
# show summary table
  t1
```

**NOTE:** This acceptance rate **INCLUDES** the 5 common surveys everyone did.

## 4.2 Survey 2

### 4.2.1 Data preview

```{r}
# structure
  #str(survey2)

# number entries
  paste('number of entries:',nrow(survey2))
  
# number columns
  paste('number of columns:',ncol(survey2))
  
# column names
  #colnames(survey2)

# number of unique papers surveyed
  paste('number of papers:',length(unique(survey2$Paper.ID.Code)))
  
# number of observers
  paste('number of observers:',length(unique(survey2$Coder.ID)))
  
# list of observers
  paste('observer names:')
  paste(unique(survey2$Coder.ID),collapse="; ")
```

We also found an article mistakenly surveyed. This will be removed from all analyses, as of a decision on 5-15-2020.

```{r}
# drop entries completely (invalid papers)
  survey2 <- survey2[!survey2$Paper.ID.Code==6005,]
```

Edit survey ID numbers

```{r}
# R_Chen paper ID error
  #survey2$Paper.ID.Code[survey2$Paper.ID.Code==109] <- 3924
```

Remove observer whose papers were completely redone by another observer (similar to Survey 1).

```{r}
# drop entries completely (invalid papers)
  survey2 <- survey2[!survey2$Coder.ID=='R_Chen',]
```

### 4.2.2 Edit columns

Drop empty columns in the dataset

```{r}
# drop blank columns
  survey2 <-  subset(survey2, select = -c(X,X.1,X.2,X.3,X.4,X.5,X.6))
```

Edit column names (too long!)

```{r}
# get colnames
  #colnames(survey2)

# change colnames (order matters!!!!!!!!)
  colnames(survey2) <- c("timestamp",
                         "coder_id",
                         "paper_id",
                         "year_pub",
                         "author",
                         "taxa_list",
                         "habitat",
                         "num_countries",
                         "list_countries",
                         "list_continents",
                         "scale_entire",
                         "num_time_pd",
                         "year_start",
                         "year_end",
                         "temp_res",
                         "temp_res_unk",
                         "meta_var_type",
                         "b4_dur_after",
                         "peri_tele_flows",
                         "tele_cat",
                         "peri_tele_sep",
                         "peri_tele_sep_unk",
                         "data_source_biodiv",
                         "data_source_meta",
                         "data_type_biodiv",
                         "data_type_meta",
                         "scale_biodiv",
                         "comments",
                         "num_biodiv_metrics",
                         "list_biodiv_metrics",
                         "further_discuss",
                         "biodiv_countries")
```

Change structure (if needed)

```{r}
# check structure
  #str(survey2)
```

### 4.2.3 Combine with Chinese version of the survey entries

First, convert timestamps, since different formats.

```{r}
# convert timestamp to POSIX format
  survey2$timestamp <- lubridate::mdy_hms(as.character(survey2$timestamp),
                                          truncated=3)

  cnS2$timestamp <- lubridate::mdy_hms(as.character(cnS2$timestamp),
                                          truncated=3)
```

Then, check column names.

```{r}
# check column name matching. If mismatch is listed, correct before binding
  cat('column mismatch test:\n')
  setdiff(colnames(survey2),colnames(cnS2))
  setdiff(colnames(cnS2),colnames(survey2))
  ncol(survey2)==ncol(cnS2)
```

Bind rows.

**NOTE:** structure for those matching column names have to be the same; also, any missing columns will be filled with NA.

```{r, echo=FALSE, eval=FALSE}
# TEMPORARY EDIT HERE UNTIL FIXES ARE MADE
  # cnS2$scale_entire <- as.factor(cnS2$scale_entire)
  # answers <- c('Yes','No')
  # cnS2$further_discuss <- mapvalues(cnS2$further_discuss,from = 1:2,to=answers)
  # cnS2$peri_tele_sep <- as.factor(cnS2$peri_tele_sep)
  # cnS2$meta_var_type <- as.factor(cnS2$meta_var_type)
```

```{r}
# combine
  survey2 <- bind_rows(survey2,cnS2)
```

### 4.2.4 Edit name errors

Name formatting matters for sorting data by timestamp. We will repeat this for the other surveys, too. 

```{r}
# change dashes to underscores
  survey2$coder_id <- gsub("-", "_", survey2$coder_id)

# change first to cap and last names to cap and lower case
  survey2$coder_id <- str_to_title(gsub("_"," ",survey2$coder_id))
  survey2$coder_id <- gsub(" ","_",survey2$coder_id)
  survey2$coder_id <- gsub("Mg","MG",survey2$coder_id)
  survey2$coder_id <- gsub("Herberger","Herzberger",survey2$coder_id)

# show updated list
  paste('observer names:')
  paste(unique(survey2$coder_id),collapse="; ")
```

### 4.2.5 Format dates and extract unique papers

We will repeat this for the other surveys, too. 

Sort and extract entry by latest timestamp. The resulting number of entries should match the number of unique study IDs across coders. If false, the error needs to be found in order to proceed.

```{r}
# get original length
  a <- length(unique(c(survey2$paper_id,survey2$coder_id)))

# select latest data entries according to paper ID, observer and timestamp
  survey2 <-  survey2 %>% 
                # Within each grouping of col 1 and col 2...
                group_by(paper_id, coder_id) %>% 
                # Sort rows by descending order of timestamp...
                # (extra step helpful in checking what happens here)
                arrange(paper_id, coder_id, desc(timestamp)) %>% 
                # Pick the latest time
                slice(which.max(timestamp)) %>% 
                # ungroup
                ungroup()
  
# check if length matches
  b <- length(unique(c(survey2$paper_id,survey2$coder_id)))
  paste('test:', a == b)
  
# how many?
  paste('count:', b)
```

### 4.2.6 Comparing matching eligible papers from survey 1 with survey 2 completion

Here we are making the assumption that all of survey 2 is dependent on survey 1. We had instances where observers began surveys 2 and 3 and then realized the paper wasn't eligible after all. They were then instructed to fill out survey 1 again and change the responses accordingly.

First we manually remove any survey 2 papers that were manually rejected above.

```{r}
# change to numeric
  survey2$paper_id <- as.character(survey2$paper_id)

# subset
  s2_keep1 <- survey2 %>% 
             # manual rejections (from list above)
               subset(!grepl(paste(manual,collapse="$|^"), 
                            paper_id)) %>% 
               subset(((paper_id!=1488 & coder_id!='Y_Li')|
                        (coder_id!='Y_Li'& paper_id!=3924)))

# subset separately for some other errors
  s2_keep2 <- survey2 %>% 
              subset(((coder_id=='Y_Li'& paper_id==813) |
                      (coder_id=='Y_Li'& paper_id==2222) |
                      (coder_id=='Y_Li'& paper_id==6655)))
  
# combine
  survey2 <- rbind(s2_keep1,s2_keep2)  
    
# get updated summary
  paste('number of entries:',nrow(survey2))
  paste('number of papers:',length(unique(survey2$paper_id)))
  paste('number of observers:',length(unique(survey2$coder_id)))
```

Then, we make edits for wrong paper IDs (found while coding below, but edits made above in prep for new survey uploads as they come in).

```{r}
# fix wrong paper ID
  #survey2$paper_id[survey2$paper_id==77 & survey2$coder_id=='R_Chen'] <- 6655
```

We will also investigate the differences in paper IDs, to ensure no other errors in data entry.

Does the number of entries match the number of eligible papers in survey 1?

```{r}
# test match of survey 1 with survey 2
  length(unique(s1_keep$paper_id)) == length(unique(survey2$paper_id))
```

This **NEEDS TO BE TRUE** for a complete, error-free dataset.


```{r, fig.height=3,fig.width=3}
# make list of paper IDs from survey 1 'keep' for matching
  keep_list <- unique(s1_keep$paper_id)

# make list to compare with survey 2
  s2_papers <- unique(survey2$paper_id)

# compare first and second lists
  first <- keep_list
  second <- s2_papers
  # in both, same as call: intersect(first, second)
  both <- first[first %in% second]
  # only in 'first', same as: setdiff(first, second)
  onlyfirst <- first[!first %in% second]
  # only in 'second', same as: setdiff(second, first)
  onlysecond <- second[!second %in% first]

# venn diagram with count of papers in each
  venn(list(survey1_keep = first, survey2_entries = second))
```

**Error check: which papers have a survey 2 but no survey 1???**

```{r}
# show only the second list of paper IDs (survey 2 only)
  onlysecond
```

These are a potential problem. Need to go back to survey 1 and see what the answers were for these.

Whose papers are they so we can contact the observers?

```{r}
# we compare with the entire survey 1, not just the keep list
  survey1[survey1$paper_id %in% onlysecond,]
```

It looks like a 'keep list' issue for this one.

We should also check the ones not in survey 1 at all.

```{r}
# we compare with the entire survey 2 for those not in survey 1 at all
  survey2[survey2$paper_id %in% onlysecond,]
```

```{r}
# we compare with the entire survey 1, not just the keep list
  #survey1[survey1$paper_id %in% onlyfirst,]

# save list as csv (if there are entries)
  s1_missing <- survey2[survey2$paper_id %in% onlysecond,]
  s1_missing <- s1_missing %>%
                   arrange(coder_id, paper_id, desc(paper_id)) %>%
                   subset(select=c(coder_id,paper_id,year_pub,author))
  # conditional save
    if (nrow(s1_missing)>=1){
        write.csv(s1_missing,
                  paste0(tab.check.dir,'survey1_missing.csv'),
                  row.names = FALSE)  
      }
  
# show list
  s1_missing
```

**Error check: which papers have a survey 1 but no survey 2???**

```{r}
# show only the first list of paper IDs (survey 1 only)
  onlyfirst
```

It's very likely that these are the papers that were made inelligible after starting surveys 2 or 3. However, it's best to double-check. If there is an error here in survey 2, then the entry for survey 1 would have to be modified.

Whose papers are they so we can contact the observers?

```{r}
# we compare with the entire survey 1, not just the keep list
  #survey1[survey1$paper_id %in% onlyfirst,]

# save list as csv (if there are entries)
  s2_missing <- survey1[survey1$paper_id %in% onlyfirst,]
  s2_missing <- s2_missing %>%
                   arrange(coder_id, paper_id, desc(paper_id)) %>%
                   subset(select=c(coder_id,paper_id,year_pub,author,status))
  # conditional save
    if (nrow(s2_missing)>=1){
        write.csv(s2_missing,
                  paste0(tab.check.dir,'survey2_missing.csv'),
                  row.names = FALSE)  
      }
  
# show list
  s2_missing
```


The 'both' section is fine and no need to investigate. They would be more of an issue if there's no survey 3 to match survey 2 (to do later). 

### 4.2.7 Extract data summaries for core team.

```{r}
# list of completed surveys
  s2_complete <- survey2 %>% 
                 arrange(coder_id, paper_id, desc(timestamp)) %>% 
                 subset(select=c(timestamp,coder_id,paper_id))

# summaries per person
  t2 <- ddply(survey2,
            .(coder_id),
            summarize,
            # total count of papers per person     
            s2_papers=length(unique(paper_id)),
            # last date someone worked
            s2_last_date=max(timestamp)
            )
  
# S1_S2 mismatches per observer
  only2 <- survey2[survey2$paper_id %in% onlysecond,]
  only1 <- survey1[survey1$paper_id %in% onlyfirst,]
  only2 <- ddply(only2,.(coder_id),summarize,
                 num_s2_no_s1=length(unique(paper_id)))
  only1 <- ddply(only1,.(coder_id),summarize,
                 num_s1_no_s2=length(unique(paper_id)))
  
# add to table (conditional on whether there are entries)
  if (nrow(only1)>=1){
    t2 <- left_join(t2,only1,by="coder_id")
    } else {
      t2$num_s1_no_s2 <- NA # no entries, so leave blank
    }
  if (nrow(only2)>=1){
    t2 <- left_join(t2,only2,by="coder_id")
    } else {
      t2$num_s2_no_s1 <- NA # no entries, so leave blank
    }
  
# save as CSVs
  write.csv(s2_complete,
            paste0(tab.check.dir,'survey2_papers_observers.csv'),
            row.names = FALSE)
  write.csv(t2,
            paste0(tab.check.dir,'survey2_progress.csv'),
            row.names = FALSE)
  
# display progress here
  t2
```

## 4.3 Survey 3

### 4.3.1 Data preview

```{r}
# structure
  #str(survey3)

# number entries
  paste('number of entries:',nrow(survey3))
  
# number columns
  paste('number of columns:',ncol(survey3))
  
# column names
  #colnames(survey3)

# number of unique papers surveyed
  paste('number of papers:',length(unique(survey3$Study.ID)))
  
# number of observers
  paste('number of observers:',length(unique(survey3$Coder.ID)))
  
# list of observers
  paste('observer names:')
  paste(unique(survey3$Coder.ID),collapse="; ")
```

Remove observer whose papers were completely redone by another observer (similar to Survey 1).

```{r}
# drop entries completely (invalid papers)
  survey3 <- survey3[!survey3$Coder.ID=='R_Chen',]
```

### 4.3.2 Edit columns

Edit column names (too long!)

```{r}
# get colnames
  #colnames(survey3)

# change colnames (order matters!!!!!!!!)
  colnames(survey3) <- c("timestamp",
                         "paper_id",
                         "taxa",
                         "entry_id",
                         "taxon_or_func",
                         "biodiv_cat_1sp",
                         "biodiv_cat_multsp",
                         "biodiv_cat_habitat", 
                         "scale",
                         "biodiv_temp_res",
                         "effect",
                         "significant",
                         "p_value",
                         "notes",
                         "coder_id")
```

Change structure (if needed)

```{r}
# check structure
  #str(survey3)
```

### 4.3.3 Combine with Chinese version of the survey entries

First, convert timestamps, since different formats.

```{r}
# convert timestamp to POSIX format
  survey3$timestamp <- lubridate::mdy_hms(as.character(survey3$timestamp),
                                          truncated=3)

  cnS3$timestamp <- lubridate::ymd_hms(as.character(cnS3$timestamp),
                                          truncated=3)
```

Then, check column names.

```{r}
# check column name matching. If mismatch is listed, correct before binding
  cat('column mismatch test:\n')
  setdiff(colnames(survey3),colnames(cnS3))
  setdiff(colnames(cnS3),colnames(survey3))
  paste('ncol Survey3:',ncol(survey3))
  paste('ncol cnSurvey3:',ncol(cnS3))
```

Bind rows.

**NOTE:** structure for those matching column names have to be the same; also, any missing columns will be filled with NA.

```{r}
# TEMPORARY EDIT HERE UNTIL FIXES ARE MADE
  # cnS3$unique_id <- as.factor(cnS3$unique_id)
  # cnS3$significant <- as.factor(cnS3$significant)

# fix structures to ensure matching
  cnS3$entry_id <- as.factor(cnS3$entry_id)
  cnS3$p_value <- as.character(cnS3$p_value)
  survey3$p_value <- as.character(survey3$p_value)

# combine
  survey3 <- bind_rows(survey3,cnS3)
```

### 4.3.4 Edit name errors

Name formatting matters for sorting data by timestamp. We will repeat this for the other surveys, too. 

```{r}
# change dashes/periods to underscores
  survey3$coder_id <- gsub("-", "_", survey3$coder_id)
  survey3$coder_id <- gsub("\\.", "_", survey3$coder_id)

# change first to cap and last names to cap and lower case
  survey3$coder_id <- str_to_title(gsub("_"," ",survey3$coder_id))
  survey3$coder_id <- gsub(" ","_",survey3$coder_id)
  survey3$coder_id <- gsub("Mg","MG",survey3$coder_id)

# show updated list
  paste('observer names:')
  paste(unique(survey3$coder_id),collapse="; ")
```

### 4.3.5 Format dates and extract unique papers and entries

We will repeat this for the other surveys, too. 

Sort and extract entry by latest timestamp. The resulting number of entries should match the number of unique study IDs across coders. 

First, double-check that all timestamps are indeed unique and not just copy-paste.

```{r}
# test all entries have unique timestamps
  a <- nrow(survey3)
  b <- length(unique(survey3$timestamp))
  paste('test:', a == b)
```



```{r, eval=FALSE, echo=FALSE}
### NOT SURE IF THIS MATTERS. NEED TO CHECK. ####

# There are errors in this part, where we don't have observer ID numbers or paper/entry ID numbers... This would also be an issue when filtering.

# # get original length
#   a <- length(unique(c(survey3$paper_id,
#                        survey3$coder_id,
#                        survey3$entry_id,
#                        survey3$taxa)))
# 
# # select latest data entries according to paper ID, observer and timestamp
#   survey3 <-  survey3 %>% 
#                 # Within each grouping of col 1 and col 2...
#                 group_by(paper_id, coder_id, survey3$entry_id) %>% 
#                 # Sort rows by descending order of timestamp...
#                 # (extra step helpful in checking what happens here)
#                 arrange(paper_id, coder_id, entry_id, desc(timestamp)) %>% 
#                 # Pick the latest time
#                 slice(which.max(timestamp)) %>% 
#                 # ungroup
#                 ungroup()
#   
# # check if length matches
#   b <- length(unique(c(survey3$paper_id,survey3$coder_id)))
#   paste('test:', a == b)
#   
# # how many?
#   paste('count:', a)
```

### 4.3.6 Removing rejected papers

As per meeting on 5/1/2020, some papers were rejected after completing other surveys beyond survey 1. We thus remove them here to avoid any further confusion.

```{r}
# correct ID number of a rejected paper
  # paperID 22, entry ID 22
    survey3$paper_id[survey3$coder_id=='A_Torres' &
              survey3$paper_id==22 & survey3$entry_id==22] <- 411

# subset out manual rejections
  s3_keep1 <- survey3 %>% 
             # manual rejections (from list above)
               subset(!grepl(paste(manual,collapse="$|^"), 
                           paper_id)) %>% 
               subset(((paper_id!=1488 & coder_id!='Y_Li')|
                      (coder_id!='Y_Li'& paper_id!=3924)))

# subset separately for some other errors
  s3_keep2 <- survey3 %>% 
              subset(((coder_id=='Y_Li'& paper_id==813) |
                      (coder_id=='Y_Li'& paper_id==6655) |
                      (coder_id=='Y_Li'& paper_id==2222) |
                      (coder_id=='X_Wu'& paper_id==1299)))
  
# combine
  survey3 <- rbind(s3_keep1,s3_keep2) 

# get updated summary
  paste('number of entries:', nrow(survey3))
  paste('number of papers:', length(unique(survey3$paper_id)))
  paste('number of observers:', length(unique(survey3$coder_id)))
```

### 4.3.7 Correcting known errors

Then, we nake edit for observer errors (found while coding below, but edits made above in prep for new survey uploads as they come in). The following corrections are from a group meeting on 5-13-2020,and from email inquiries on 5-18-2020 to coauthors.

```{r}
# A_Torres errors
  # paper ID 1488, entry ID 4
    survey3$paper_id[survey3$coder_id=='A_Torres' &
              survey3$paper_id==4 & survey3$entry_id==1488] <- 1488
    survey3$entry_id[survey3$coder_id=='A_Torres' &
              survey3$paper_id==1488 & survey3$entry_id==1488] <- 4
  # paperID 3924, entry ID 5
    survey3$entry_id[survey3$coder_id=='A_Torres' &
              survey3$paper_id==5 & survey3$entry_id==''] <- 5
    survey3$paper_id[survey3$coder_id=='A_Torres' &
              survey3$paper_id==5 & survey3$entry_id==5] <- 3924
  # paperID 3924, entry ID 6
    survey3$entry_id[survey3$coder_id=='A_Torres' &
              survey3$paper_id==3924 & survey3$entry_id==''] <- 6
    
# E_Dean errors
  # paperID 22, entry ID 22
    survey3$paper_id[survey3$coder_id=='E_Dean' &
              survey3$paper_id==4 & survey3$entry_id==4] <- 2998
    survey3$paper_id[survey3$coder_id=='E_Dean' &
              survey3$paper_id==5 & survey3$entry_id==5] <- 5844
    survey3$entry_id[survey3$coder_id=='' &
                    survey3$entry_id=='Tayra (Eira barbara)'] <- 1
# C_Hovis errors
  # paper ID 6655
    survey3$paper_id[survey3$coder_id=='C_Hovis' & survey3$paper_id==6666] <- 6655
    
# MC_Chung errors
  survey3$coder_id[survey3$paper_id==45] <- "MG_Chung"
  survey3$coder_id[survey3$paper_id==654] <- "MG_Chung"

# R_Chen errors
  # paper ID 6655 (from survey 1 correction)
    # survey3$paper_id[survey3$coder_id=='R_Chen' & survey3$paper_id==77] <- 6655
    # survey3$paper_id[survey3$coder_id=='R_Chen' & survey3$paper_id==109] <- 3924

# X_Wu errors
  # fill blank paper ID
    survey3$paper_id[survey3$coder_id=='X_Wu' & survey3$paper_id==3] <- 1299  
    
# rejections from email confirmation
  # survey3 <- survey3[!(survey3$paper_id==0000 & survey3$coder_id=='____'),]

```

We also correct for unknown coders, organized by timestamp.

```{r}
# Y_Zhang
  int <- interval(ymd_hms("2020-03-17 13:08:00"), ymd_hms("2020-03-17 13:10:59"))
  survey3$coder_id[survey3$timestamp %within% int] <- 'Y_Zhang'
  
# K_Kapsar
  int <- interval(ymd_hms("2020-03-17 14:38:00"), ymd_hms("2020-03-17 14:38:59"))
  int2 <- interval(ymd_hms("2020-03-17 11:53:00"), ymd_hms("2020-03-17 11:53:59"))
  int3 <- interval(ymd_hms("2020-03-17 12:31:40"), ymd_hms("2020-03-17 12:32:59"))
  survey3$coder_id[survey3$timestamp %within% int |
                   survey3$timestamp %within% int2 |
                   survey3$timestamp %within% int3] <- 'K_Kapsar'
  
# A_Herzberger
  int <- interval(ymd_hms("2020-03-17 10:58:00"), ymd_hms("2020-03-17 10:59:59"))
  int2 <- interval(ymd_hms("2020-03-17 12:53:30"), ymd_hms("2020-03-17 12:55:59"))
  survey3$coder_id[survey3$timestamp %within% int|
                   survey3$timestamp %within% int2] <- 'A_Herzberger'
  
# E_Dean
  int <- interval(ymd_hms("2020-03-15 13:32:00"), ymd_hms("2020-03-15 13:33:59"))
  int2 <- interval(ymd_hms("2020-03-15 12:33:00"), ymd_hms("2020-03-15 12:33:59"))
  int3 <- interval(ymd_hms("2020-03-15 14:41:00"), ymd_hms("2020-03-15 14:48:59"))
  survey3$coder_id[survey3$timestamp %within% int |
                   survey3$timestamp %within% int2|
                   survey3$timestamp %within% int3] <- 'E_Dean'
```

### 4.3.8 Comparing matching eligible papers from survey 1 with survey 3 completion

Here we are making the assumption that all of survey 3 is dependent on survey 1. We had instances where observers began surveys 2 and 3 and then realized the paper wasn't eligible after all. They were then instructed to fill out survey 1 again and change the responses accordingly.

Does the number of entries match the number of eligible papers in survey 1?

```{r}
# test match of survey 1 with survey 3
  length(unique(s1_keep$paper_id)) == length(unique(survey3$paper_id))
```

Does the number of unique papers match the number of papers in survey 2?

```{r}
# test match of survey 2 with survey 3
  length(unique(survey2$paper_id)) == length(unique(survey3$paper_id))
```


We will also investigate the differences in paper IDs, to ensure no other errors in data entry.

```{r, fig.height=3,fig.width=3}
# make list of paper IDs from survey 1 'keep' for matching
  keep_list <- unique(s1_keep$paper_id)

# make list to compare with survey 2
  s3_papers <- unique(survey3$paper_id)

# compare first and third lists
  first <- keep_list
  third <- s3_papers
  # in both, same as call: intersect(first, third)
  both <- first[first %in% third]
  # only in 'first', same as: setdiff(first, third)
  onlyfirst <- first[!first %in% third]
  # only in 'third', same as: setdiff(third, first)
  onlythird <- third[!third %in% first]

# venn diagram with count of papers in each
  venn(list(survey1_keep = first, survey3_entries = third))
```

**Error check: which papers have a survey 3 but no survey 1???**

```{r}
# show only the third list of paper IDs (survey 3 only)
  onlythird
```

These are a potential problem. Need to go back to survey 1 and see what the answers were for these.

Whose papers are they so we can contact the observers?

```{r}
# we compare with the entire survey 1, not just the keep list
  a <- survey1[survey1$paper_id %in% onlythird,]
```

It looks like a 'keep list' issue for this one.

We should also check the ones not in survey 1 at all.

```{r}
# we compare with the entire survey 3 for those not in survey 1 at all
  survey3[survey3$paper_id %in% onlythird,]
```

```{r}
# save list as csv (if there are entries)
  s31_missing <- survey3[survey3$paper_id %in% onlythird,]
  s31_missing <- s31_missing %>%
                   arrange(coder_id, paper_id) %>%
                   subset(select=c(coder_id,paper_id,entry_id))
  # conditional save
    if (nrow(s31_missing)>=1){
        write.csv(s31_missing,
                  paste0(tab.check.dir,'survey3_missing_survey1.csv'),
                  row.names = FALSE)  
    }
  
# show list
  s31_missing
```

**Error check: which papers have a survey 1 but no survey 3???**

```{r}
# show only the first list of paper IDs (survey 1 only)
  onlyfirst
```

It's very likely that these are the papers that were made inelligible after starting surveys 2 or 3. However, it's best to double-check. If there is an error here in survey 3, then the entry for survey 1 would have to be modified.

Whose papers are they so we can contact the observers?

```{r}
# we compare with the entire survey 1, not just the keep list
  #survey1[survey1$paper_id %in% onlyfirst,]

# save list as csv (if there are entries)
  s3_missing <- survey1[survey1$paper_id %in% onlyfirst,]
  s3_missing <- s3_missing %>%
                   arrange(coder_id, paper_id, desc(paper_id)) %>%
                   subset(select=c(coder_id,paper_id,year_pub,author,status))
  
  # conditional save
    if (nrow(s3_missing)>=1){
        write.csv(s3_missing,
                  paste0(tab.check.dir,'survey3_missing_from_1.csv'),
                  row.names = FALSE)  
    }
  
# show list
  s3_missing
```

The 'both' section is fine and no need to investigate. 

Get summary data to add to core team summary below.

```{r}
# S1_s3 mismatches per observer
  only3 <- survey3[survey3$paper_id %in% onlythird,]
  only1 <- survey1[survey1$paper_id %in% onlyfirst,]
  only3 <- ddply(only3,.(coder_id),summarize,
                 num_s3_no_s1=length(unique(paper_id)))
  only1 <- ddply(only1,.(coder_id),summarize,
                 num_s1_no_s3=length(unique(paper_id)))
```

### 4.3.9 Comparing matching eligible papers from survey 2 with survey 3 completion

You can't have a survey 3 without a survey 2. Time to check it out.

```{r, fig.height=3,fig.width=3}
# make list of paper IDs from survey 1 'keep' for matching
  keep_list <- unique(survey2$paper_id)

# make list to compare with survey 2
  s3_papers <- unique(survey3$paper_id)

# compare second and third lists
  second <- keep_list
  third <- s3_papers
  # in both, same as call: intersect(second, third)
  both <- second[second %in% third]
  # only in 'second', same as: setdiff(second, third)
  onlysecond <- second[!second %in% third]
  # only in 'third', same as: setdiff(third, second)
  onlythird <- third[!third %in% second]

# venn diagram with count of papers in each
  venn(list(survey2_entries = second, survey3_entries = third))
```

**Error check: which papers have a survey 3 but no survey 2???**

```{r}
# show only the third list of paper IDs (survey 3 only)
  onlythird
```

These are a potential problem. Need to go back to survey 2 and see what the answers were for these.

Whose papers are they so we can contact the observers?

```{r}
# we compare with the entire survey 2, not just the keep list
  survey2[survey2$paper_id %in% onlythird,]
```

It looks like a 'keep list' issue for this one.

We should also check the ones not in survey 2 at all.

```{r}
# we compare with the entire survey 3 for those not in survey 2 at all
  survey3[survey3$paper_id %in% onlythird,]
```

```{r}
# save list as csv (only if there are entries)
  s32_missing <- survey3[survey3$paper_id %in% onlythird,]
  s32_missing <- s32_missing %>%
                   arrange(coder_id, paper_id) %>%
                   subset(select=c(coder_id,paper_id,entry_id))
  # conditional save
    if (nrow(s32_missing)>=1){
    write.csv(s32_missing,
              paste0(tab.check.dir,'survey3_missing_survey2.csv'),
              row.names = FALSE)
    }
  
# show list
  s32_missing
```

**Error check: which papers have a survey 2 but no survey 3???**

```{r}
# show only the second list of paper IDs (survey 2 only)
  onlysecond
```


```{r}
# we compare with the entire survey 2, not just the keep list
  survey2[survey2$paper_id %in% onlysecond,]
```

Export list to contact observers.

```{r}
# save list as csv (if there are entries)
  s3_missing <- survey1[survey1$paper_id %in% onlysecond,]
  s3_missing <- s3_missing %>%
                   arrange(coder_id, paper_id, desc(paper_id)) %>%
                   subset(select=c(coder_id,paper_id,year_pub,author,status))
    # conditional save
    if (nrow(s3_missing)>=1){
        write.csv(s3_missing,
                  paste0(tab.check.dir,'survey3_missing_from_2.csv'),
                  row.names = FALSE)
    }

# show list
  s3_missing
```

### 4.3.10 Check NA entry ID fields

```{r}
# show empty entry IDs
  survey3[survey3$entry_id=='',]
```

```{r}
# get unique entry IDs from list and add value
  pID_list <- survey3[survey3$entry_id=='',]$paper_id
  cID_list <- survey3[survey3$entry_id=='',]$coder_id

  for (i in 1:length(pID_list)){
    # activate to preview
    # print(survey3[survey3$paper_id==pID_list[[i]] &
    #                 survey3$coder_id==cID_list[[i]],])
  }
```

```{r}
# edit entry IDs manually
  survey3$entry_id[survey3$entry_id==''& survey3$coder_id==cID_list[[1]]] <- 1
  survey3$entry_id[survey3$entry_id==''& survey3$coder_id==cID_list[[2]]] <- 2
  survey3$entry_id[survey3$entry_id==''& survey3$coder_id==cID_list[[3]]] <- 6
  survey3$entry_id[survey3$entry_id==''& survey3$coder_id==cID_list[[4]]] <- 4
  survey3$entry_id[survey3$entry_id==''& survey3$coder_id==cID_list[[5]]] <- 2
  survey3$entry_id[survey3$entry_id==''& survey3$coder_id==cID_list[[6]]] <- 1
  survey3$entry[survey3$entry_id==''& survey3$paper_id==pID_list[[8]] & 
                     survey3$coder_id==cID_list[[8]]][1] <- 7
  survey3$entry[survey3$entry_id==''& survey3$paper_id==pID_list[[8]] & 
                     survey3$coder_id==cID_list[[8]]][2] <- 8
  survey3$entry_id[survey3$entry_id==''& survey3$paper_id==pID_list[[9]] & 
                     survey3$coder_id==cID_list[[9]]] <- 1
```

### 4.3.11 Find empty coder ID fields

```{r}
# show empty coder IDs
  survey3[survey3$coder_id=='',]
```

### Check NA coder ID fields

Check survey assignments from previous surveys to get coder ID. This was especially a problem for ID 3456, 3924 and 6655 since they were assigned to all, but was solved above using timestamps.

```{r}
# get paper IDs from list
  pID_list <- unique(survey3$paper_id[survey3$coder_id==''])

# show list
  pID_list
```


### 4.3.12 Data summaries for core team

```{r}
# list of completed surveys
  s3_complete <- survey3 %>% 
                 arrange(coder_id, paper_id,desc(entry_id)) %>% 
                 subset(select=c(timestamp,coder_id,paper_id,entry_id))

# summaries per person
  t3 <- ddply(survey3,
            .(coder_id),
            summarize,
            # total count of papers per person     
            s3_papers=length(unique(paper_id)),
            # total count of entries per person     
            s3_entries=length(entry_id),
            # last date someone worked
            s3_last_date=max(timestamp)
            )
  
# S2_s3 mismatches per observer
  s3no2 <- survey3[survey3$paper_id %in% onlythird,]
  s3no2 <- ddply(s3no2,.(coder_id),summarize,
                 num_s3_no_s2=length(unique(paper_id)))
  s2no3 <- survey2[survey2$paper_id %in% onlysecond,]
  s2no3 <- ddply(s2no3,.(coder_id),summarize,
                 num_s2_no_s3=length(unique(paper_id)))
   
# add to table (conditional on whether there are entries)
  if (nrow(only1)>=1){
    t3 <- left_join(t3,only1,by="coder_id")
    } else {
      t3$num_s1_no_s3 <- NA # no entries, so leave blank
    }
  if (nrow(only3)>=1){
    t3 <- left_join(t3,only3,by="coder_id")
    } else {
      t3$num_s3_no_s1 <- NA # no entries, so leave blank
    }
  if (nrow(s3no2)>=1){
    t3 <- left_join(t3,s3no2,by="coder_id")
    } else {
      t3$num_s3_no_s2 <- NA # no entries, so leave blank
    }
  if (nrow(s2no3)>=1){
    t3 <- left_join(t3,s2no3,by="coder_id")
    } else {
      t3$num_s2_no_s3 <- NA # no entries, so leave blank
    }
                   
# save as CSVs
  write.csv(s3_complete,
            paste0(tab.check.dir,'survey3_papers_observers.csv'),
            row.names = FALSE)
  write.csv(t3,
            paste0(tab.check.dir,'survey3_progress.csv'),
            row.names = FALSE)
  
# show progress table
  t3
```

# 5. Export surveys for cleanup

## 5.1 Exports for manual checking.

```{r}
# exports for manual check of study IDs
  write.csv(survey1,paste0(tab.check.dir,
                           'survey1_for_manual_check.csv'),row.names=FALSE)
  write.csv(survey2,paste0(tab.check.dir,
                           'survey2_for_manual_check.csv'),row.names=FALSE)
  write.csv(survey3,paste0(tab.check.dir,
                           'survey3_for_manual_check.csv'),row.names=FALSE)
```

## 5.2 Extract 5 common paper survey responses

The 5 common surveys will be analyzed separately. The most common response will be used as the final 'answers', and then appended to the rest of surveys 1 thru 3 for the other analytical portions of the study.

```{r}
# list of common papers to extract
  commons <- c(6655,2577,3924,3456,1488)

# remove from full survey
  s1_full <- survey1 %>%
               # remove by list
                 subset(!grepl(paste(commons,collapse="|"),paper_id))
  s2_full <- survey2 %>%
               # remove by list
                 subset(!grepl(paste(commons,collapse="|"),paper_id))
  s3_full <- survey3 %>%
               # remove by list
                 subset(!grepl(paste(commons,collapse="|"),paper_id))

# anti-join to make list of rejected papers
  survey1c <- anti_join(survey1,s1_full)
  survey2c <- anti_join(survey2,s2_full)
  survey3c <- anti_join(survey3,s3_full)
  
# show length of accepted and rejected
  # paste('number accepted papers:', length(unique(s1_keep$paper_id)));
  # paste('number rejected papers:', length(unique(s1_reject$paper_id)))
  
# check if length of objects match survey form
  paste('test1:', nrow(survey1c)+nrow(s1_full) == nrow(survey1))
  paste('test2:', nrow(survey2c)+nrow(s2_full) == nrow(survey2))
  paste('test3:', nrow(survey3c)+nrow(s3_full) == nrow(survey3))
```

All common surveys have been assessed as of 6/4/2020, so no further work on these is needed.

```{r, eval=TRUE}
# make subfolder
  dir.create(paste0(dat.dir,'\\common_papers'),recursive=TRUE)

# exports for common paper surveys
  write.csv(survey1c,paste0(dat.dir,'\\common_papers\\',
                            'survey1_common_papers.csv'),row.names=FALSE)
  write.csv(survey2c,paste0(dat.dir,'\\common_papers\\',
                            'survey2_common_papers.csv'),row.names=FALSE)
  write.csv(survey3c,paste0(dat.dir,'\\common_papers\\',
                            'survey3_common_papers.csv'),row.names=FALSE)
```

## 5.3 Extract all survey responses

```{r}
# make subfolder
  dir.create(paste0(dat.dir,'\\survey_cleanup'),recursive=TRUE)

# exports for survey cleanup
  write.csv(s1_full,paste0(dat.dir,'\\survey_cleanup\\',
                          'survey1_for_cleanup.csv'),row.names=FALSE)
  write.csv(s2_full,paste0(dat.dir,'\\survey_cleanup\\',
                          'survey2_for_cleanup.csv'),row.names=FALSE)
  write.csv(s3_full,paste0(dat.dir,'\\survey_cleanup\\',
                          'survey3_for_cleanup.csv'),row.names=FALSE)
```

## 5.4 Extract column names for dictionary

```{r, eval=FALSE}
# extract column names
  s1_coln <- data.frame(column_name=colnames(survey1), question='NA')
  s2_coln <- data.frame(column_name=colnames(survey2), question='NA')
  s3_coln <- data.frame(column_name=colnames(survey3), question='NA')
  
# make subfolder
  dir.create(paste0(dat.dir,'\\survey_colnames'),recursive=TRUE)

# exports for survey cleanup
  write.csv(s1_coln,paste0(dat.dir,'\\survey_colnames\\',
                          'survey1_colnames.csv'),row.names=FALSE)
  write.csv(s2_coln,paste0(dat.dir,'\\survey_colnames\\',
                          'survey2_colnames.csv'),row.names=FALSE)
  write.csv(s3_coln,paste0(dat.dir,'\\survey_colnames\\',
                          'survey3_colnames.csv'),row.names=FALSE)
```

# 6. Save workspace

```{r,cache=TRUE,cache.comments=FALSE}
# save workspace to load later if needed
  save.image("RawSurveyPrep.RData")
```

-----------------------------------------------------------------------------------